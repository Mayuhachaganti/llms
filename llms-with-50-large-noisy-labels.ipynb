{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":2415872,"sourceType":"datasetVersion","datasetId":1461623}],"dockerImageVersionId":30178,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Amazon Reviews Modelling Using BERT with Noisy Labels","metadata":{}},{"cell_type":"markdown","source":"# 1. Install and Import Necessary Libray For NLP and Sentiment Analysis","metadata":{}},{"cell_type":"code","source":"%%capture\n!pip install stopwords\n!pip install flair\n!pip install nltk\n!pip install swifter","metadata":{"execution":{"iopub.status.busy":"2024-08-22T14:21:58.752156Z","iopub.execute_input":"2024-08-22T14:21:58.752434Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install --upgrade pyarrow","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport re\nimport nltk\nfrom nltk.corpus import stopwords\nfrom wordcloud import WordCloud\nimport matplotlib.pyplot as plt\nimport random as rn\nimport seaborn as sns\nfrom plotly import graph_objs as go\nimport plotly.express as px\nimport plotly.figure_factory as ff\nfrom collections import Counter\nfrom PIL import Image\n\nfrom imblearn.over_sampling import RandomOverSampler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfTransformer\n\npd.options.display.max_rows = None\nseed=40","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":" ","metadata":{}},{"cell_type":"markdown","source":"# 2. Import Amazon Reviews Dataset","metadata":{}},{"cell_type":"markdown","source":"### taking 10% of the data to analyze and to be trained.","metadata":{}},{"cell_type":"code","source":"rn.seed(a=40)\n\nreview = pd.read_csv('../input/amazon-product-reviews/Reviews.csv')\nreview.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"review.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### grouping the scores into positive and negative sentiment","metadata":{}},{"cell_type":"code","source":"def score_round(x):\n    if x>=3:\n        return 1\n    else: \n        return 0\n    \nreview['review_score']= review.Score.apply(score_round)\nreview = review.rename(columns={'Text':'review_text'})\ndf=review.copy()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['review_score'].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"zeroes=df[review['review_score']==0].head(30000)\nones=df[review['review_score']==1].head(20000)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"d=pd.concat([zeroes,ones])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"d.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['review_score'].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df=d.copy()\ndf=df.sample(frac=1).reset_index(drop=True)\ndf.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#len(indices_to_shuffle)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"d=df.copy()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"d['review_score'].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''for idx in indices_to_shuffle:\n    if df['review_score'].loc[idx]==1:\n        df.at[idx,'review_score']=0\n    elif df['review_score'].loc[idx]==0:\n        df.at[idx,'review_score']=1\n    else:\n        print('no invalid value')'''","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['review_score'].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"review=df.copy()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# review.review_score.replace(\"positive\" , 1 , inplace = True)\n# review.review_score.replace(\"negative\" , 0 , inplace = True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### making sure that text review data type is string","metadata":{}},{"cell_type":"code","source":"review.review_text = review.review_text.astype('str')\nreview.Summary = review.Summary.astype('str')\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 3.  Distribution of Reviews in Each Sentiment","metadata":{}},{"cell_type":"code","source":"temp = review.groupby('review_score').count()['review_text'].reset_index().sort_values(by='review_text',ascending=False)\ntemp.style.background_gradient(cmap='Purples')\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### the visualization of it.","metadata":{}},{"cell_type":"code","source":"sns.set_theme(style='whitegrid')\nsns.set(rc = {'figure.figsize':(13,8)})\nsns.set_palette(\"pastel\")\nsns.countplot(x='review_score',data=review)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"positive = review[review['review_score']==1]\nnegative = review[review['review_score']==0]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":" ","metadata":{}},{"cell_type":"markdown","source":"# 4. Data Cleaning For Sentiment Processing","metadata":{}},{"cell_type":"code","source":"from nltk.corpus import stopwords\nfrom nltk import WordNetLemmatizer\nnltk.download('stopwords')\nfrom nltk.stem import PorterStemmer","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Null values Cleaning","metadata":{}},{"cell_type":"code","source":"review.isnull().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Duplicate Values Removing","metadata":{}},{"cell_type":"code","source":"review = review.drop_duplicates(keep='first')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"review.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### a function to clean some basic characters","metadata":{}},{"cell_type":"code","source":"def clean(raw):\n    \"\"\" Remove hyperlinks and markup \"\"\"\n    result = re.sub(\"<[a][^>]*>(.+?)</[a]>\", 'Link.', raw)\n    result = re.sub('&gt;', \"\", result)\n    result = re.sub('&#x27;', \"'\", result)\n    result = re.sub('&quot;', '\"', result)\n    result = re.sub('&#x2F;', ' ', result)\n    result = re.sub('<p>', ' ', result)\n    result = re.sub('</i>', '', result)\n    result = re.sub('&#62;', '', result)\n    result = re.sub('<i>', ' ', result)\n    result = re.sub(\"\\n\", '', result)\n    return result","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### function to remove numbers","metadata":{}},{"cell_type":"code","source":"def remove_num(texts):\n   output = re.sub(r'\\d+', '', texts)\n   return output","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### function to remove emoji","metadata":{}},{"cell_type":"code","source":"def deEmojify(x):\n    regrex_pattern = re.compile(pattern = \"[\"\n        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           \"]+\", flags = re.UNICODE)\n    return regrex_pattern.sub(r'', x)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### function to unify whitespaces","metadata":{}},{"cell_type":"code","source":"def unify_whitespaces(x):\n    cleaned_string = re.sub(' +', ' ', x)\n    return cleaned_string ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### function to remove symbols","metadata":{}},{"cell_type":"code","source":"def remove_symbols(x):\n    cleaned_string = re.sub(r\"[^a-zA-Z0-9?!.,]+\", ' ', x)\n    return cleaned_string","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### function to remove punctuation","metadata":{}},{"cell_type":"code","source":"def remove_punctuation(text):\n    final = \"\".join(u for u in text if u not in (\"?\", \".\", \";\", \":\",  \"!\",'\"',','))\n    return final","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### function to remove stopwords","metadata":{}},{"cell_type":"code","source":"stop=set(stopwords.words(\"english\"))\nstemmer=PorterStemmer()\nlemma=WordNetLemmatizer()\n\ndef remove_stopword(text):\n   text=[word.lower() for word in text.split() if word.lower() not in stop]\n   return \" \".join(text)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### function to use stemming to normalize words","metadata":{}},{"cell_type":"code","source":"from nltk.corpus import stopwords\nfrom nltk.stem import SnowballStemmer\ndef Stemming(text):\n   stem=[]\n   stopword = stopwords.words('english')\n   snowball_stemmer = SnowballStemmer('english')\n   word_tokens = nltk.word_tokenize(text)\n   stemmed_word = [snowball_stemmer.stem(word) for word in word_tokens]\n   stem=' '.join(stemmed_word)\n   return stem","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### combining all the cleaning functions","metadata":{}},{"cell_type":"code","source":"def cleaning(df,review):\n    df[review] = df[review].apply(clean)\n    df[review] = df[review].apply(deEmojify)\n    df[review] = df[review].str.lower()\n    df[review] = df[review].apply(remove_num)\n    df[review] = df[review].apply(remove_symbols)\n    df[review] = df[review].apply(remove_punctuation)\n    df[review] = df[review].apply(remove_stopword)\n    df[review] = df[review].apply(unify_whitespaces)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cleaning(review,'review_text')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"review.isnull().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"review[['review_text']].head(20)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Now all the cleanings are done and will try to visualize the common words distribution of our review texts.","metadata":{}},{"cell_type":"code","source":"review_vis2 = review.copy()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"review_vis2['temp_list'] = review_vis2['review_text'].apply(lambda x:str(x).split())\ntop = Counter([item for sublist in review_vis2['temp_list'] for item in sublist])\ntemp = pd.DataFrame(top.most_common(20))\ntemp.columns = ['Common_words','count']\ntemp.style.background_gradient(cmap='Blues')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.set_theme(style=\"whitegrid\")\nsns.barplot(x=\"count\", y=\"Common_words\", data=temp)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### common words are 'br','like','taste'","metadata":{}},{"cell_type":"markdown","source":"# 5. BERT Tokenizing and Modelling","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix, classification_report\n\nimport tensorflow as tf\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.losses import SparseCategoricalCrossentropy\nfrom tensorflow.keras.layers import Dense, Input\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.callbacks import ModelCheckpoint\nfrom tokenizers import BertWordPieceTokenizer\n\nimport transformers\nfrom transformers import BertTokenizer, TFBertForSequenceClassification\nfrom transformers import InputExample, InputFeatures\n\nimport logging\ntransformers.logging.set_verbosity_error()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Split Data to Training, Validation, and Hold-Out Data","metadata":{}},{"cell_type":"code","source":"reviews = review[\"review_text\"].values.tolist()\nlabels = review[\"review_score\"].tolist()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# split the dataset into train, validation and holdout sets (60-20-20)\ntraining_sentences, test_sentences, training_labels, test_labels = train_test_split(reviews, labels, test_size=.4)\n\nvalidation_sentences, holdout_sentences, validation_labels, holdout_labels = train_test_split(test_sentences, test_labels, test_size=.5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"training_labels = pd.Series(training_labels)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(type(training_labels.index))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"num_rows_to_shuffle = int(0.5 * len(training_labels))\nnp.random.seed(0)  \nindices_to_shuffle = np.random.choice(training_labels.index[1:], size=num_rows_to_shuffle, replace=False)\ntraining_labels.iloc[indices_to_shuffle] = np.random.choice(training_labels, size=num_rows_to_shuffle)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(indices_to_shuffle)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Tokenize Input Data","metadata":{}},{"cell_type":"code","source":"tokenizer = BertTokenizer.from_pretrained('bert-large-cased')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Example of the tokenizer output\ntokenizer([training_sentences[0]], truncation=True,\n                            padding=True, max_length=512)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"review['review_score'].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# tokenize training, validation and hold-out Texts\n\ntrain_encodings = tokenizer(training_sentences,\n                            truncation=True,\n                            padding=True)\n\nvalidation_encodings = tokenizer(validation_sentences,\n                            truncation=True,\n                            padding=True)\n\nholdout_encodings = tokenizer(holdout_sentences,\n                            truncation=True,\n                            padding=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# convert the input encodings and labels into a TensorFlow Dataset object\n\ntrain_dataset = tf.data.Dataset.from_tensor_slices((\n                            dict(train_encodings),\n                            training_labels\n                            ));\n\nvalidation_dataset = tf.data.Dataset.from_tensor_slices((\n                            dict(validation_encodings),\n                            validation_labels\n                            ));\n\nholdout_dataset = tf.data.Dataset.from_tensor_slices((\n                            dict(holdout_encodings),\n                            holdout_labels\n                            ));","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### BERT Model Fine-Tuning","metadata":{}},{"cell_type":"code","source":"# initialize pre-trained BERT model\n\nmodel = TFBertForSequenceClassification.from_pretrained('bert-large-cased',num_labels=2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"optimizer = tf.keras.optimizers.Adam(learning_rate=2e-5, epsilon=1e-08)\nmodel.compile(optimizer=optimizer, loss=model.compute_loss, metrics=['accuracy'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\nprint(\"TensorFlow version:\", tf.__version__)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.callbacks import ModelCheckpoint\n\ncheckpoint_path = os.path.join(os.getcwd(), 'model_checkpoint.h5')\ncheckpoint_callback = ModelCheckpoint(\n    filepath=checkpoint_path, \n    save_weights_only=False,\n    save_best_only=False,            \n    monitor='val_loss',              \n    mode='min',                     \n    verbose=1,\n    save_freq='epoch'\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history = model.fit(train_dataset.shuffle(1000).batch(2),\n          epochs=1,\n          validation_data=validation_dataset.shuffle(1000).batch(2), \n                    callbacks=[checkpoint_callback], verbose=1)\nprint(\"Model Saved in first epoch\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check if the model was saved and load it\nif os.path.exists(checkpoint_path):\n    model = tf.keras.models.load_model(checkpoint_path)\n    print(f\"Loaded model from {checkpoint_path}\")\nelse:\n    print(\"No checkpoint found. Starting from scratch.\")\n\n# Continue training from the next epoch\nhistory = model.fit(train_dataset.shuffle(1000).batch(2),\n                    initial_epoch=2,  # Start from the second epoch\n                    epochs=3,  # Total number of epochs you want to train\n                    validation_data=validation_dataset.shuffle(1000).batch(2), \n                    callbacks=[checkpoint_callback],\n                    verbose=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from matplotlib.ticker import MaxNLocator\n\n# plot train and validation accuracy\n\nax = plt.figure().gca()\nplt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nax.xaxis.set_major_locator(MaxNLocator(integer=True))\nplt.legend(['train', 'val'], loc='upper left')\nplt.show()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.save_pretrained(\"./output_model\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### BERT Model's Performance Evaluation","metadata":{}},{"cell_type":"code","source":"# load the model and then evaluate it on holdout set\n\nloaded_model = TFBertForSequenceClassification.from_pretrained(\"./output_model\")\nresult = model.evaluate(holdout_dataset.batch(8))\ndict(zip(model.metrics_names, result))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### the model's accuracy 88.94% on the holdout set (other validation set)","metadata":{}},{"cell_type":"code","source":"# predict the sentiment for holdout set\n\ntf_output = loaded_model.predict(holdout_dataset.batch(8))\npred_label = tf.argmax(tf.nn.softmax(tf_output[\"logits\"], axis=1).numpy(), 1).numpy()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# confusion matrix of our predictions\n\ncm = tf.math.confusion_matrix(\n    holdout_labels, pred_label, num_classes=2, weights=None, dtype=tf.dtypes.int32,\n    name=None\n).numpy()\n\nprint(\"confusion matrix\\n\",cm)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### The model has a strong ability to correctly identify positive instances, with relatively few errors in misclassifying positives as negatives and vice versa. The high number of true positives and true negatives compared to false positives and false negatives indicates good performance in both detecting positives and rejecting negatives.","metadata":{}},{"cell_type":"code","source":"# visualize the confusion matrix\n\ncm_norm = np.around(cm.astype('float') / cm.sum(axis=1)[:, np.newaxis], decimals=2)\n\ncm_df = pd.DataFrame(cm_norm,\n                 index = [0,1], \n                 columns =[0,1])\n\nfigure = plt.figure(figsize=(4, 4))\nsns.heatmap(cm_df, annot=True, cmap=plt.cm.Blues)\nplt.tight_layout()\nplt.ylabel('True label')\nplt.xlabel('Predicted label')\nplt.show()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Precision, Recall, and F1-Score\n\ntp = cm[0][0]\nfn = cm[0][1]\n\ntn = cm[1][1]\nfp = cm[1][0]\n\nrecall = round(tp / (tp+fn), 2)\nprecision = round(tp / (tp+fp), 2)\nf1score = (2*precision*recall) / (precision + recall)\n\nprint(\"recall score:\", recall)\nprint(\"precision score:\", precision)\nprint(\"F1-Score:\",f1score)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### The model's score with f1-score of 0.909.That's a strong F1-score, indicating the model performs well in terms of precision and recall. ","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}